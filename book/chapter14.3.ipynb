{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부스팅은 여러 '기저' 분류기들을 조합해서 단일 기저 분류기에 비해 훨씬 더 나은 성능을 보이는 위원회를 구성하는 테크닉이다. <br>\n",
    "부스팅은 (그 어떤 기저 분류기 보다 월등히 좋은 성능을 내는) 위원회를 만드는 방식으로 여러 기저 분류기를 조합하는 강력한 테크닉이다.  <br>\n",
    "Boosting is a powerful technique for combining multiple base classifiers to produce a form of committee whose performance can be significantly better than that of any of the base classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 가장 널리 사용되는 부스팅 알고리즘으로서 Freund and Schapire(1996)에 의해 개발된 에이다부스트(AdaBoost)를 살펴볼 것이다. 이름은 적응적 부스팅의 약자다. <br>\n",
    "Here we describe the most widely used form of boosting algorithm called AdaBoost, short for adaptive boosting, developed by Freund and Schapire (1996)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기저 분류기들이 랜덤보다 아주 약간만 나은 성능을 보이더라도 부스팅을 적용하면 좋은 결과를 얻을 수 있다. 그렇기 때문에 이 경우의 기저 분류기들을 종종 약학습기라 지칭한다. <br>\n",
    "Boosting can give good results even if the base classifiers have a performance that is only slightly better than random, and hense sometimes the base classifiers are known as weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부스팅은 원래 분류 문제를 푸는 목적으로 만들어졌지만 회귀 문제를 푸는 데도 활용할 수 있다(Friedman, 2001). <br>\n",
    "Originally designed for solving classification problems, boosting can also be extended to regression (Friedman, 2001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 살펴본 배깅 등의 위원회 방법과 부스팅 방법 사이의 주된 차이점은 부스팅 방법에서는 기저 분류기들이 순차적으로 훈련된다는 것이다. 이 때 각 기저 분류기들은 가중된 형태의 데이터 집합을 이용해서 훈련되며, 가중 계수들은 이전 분류기의 결과에 의해 결정된다. <br>\n",
    "The principal difference between boosting and the committee methods such as bagging discussed above, is that the base classifiers are trained in sequence, and each base classifier is trained using a weighted form of the data set in which the weighting coefficient associated with each data point depends on the performance of the previous classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특히, 기본 분류기 중 하나에 의해 잘못 분류 된 포인트는 시퀀스에서 다음 분류기를 훈련하는 데 사용될 때 더 큰 가중치가 부여됩니다. <br>\n",
    "기저 분류기 중 하나에 의해 오분류된 포인트들은 배열상에서의 다음 기저 분류기의 훈련에 사용될 때 더 큰 가중치를 부여받는다. <br>\n",
    "In particular, points that are misclassified by one of the base classifiers are given greater weight when used to train the next classifier in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 분류 기가 학습되면 그림 14.1에 개략적으로 설명 된대로 가중치가 적용된 과반수 투표 방식을 통해 예측이 결합됩니다. <br>\n",
    "모든 분류기들을 훈련하고 나면 각 분류기들의 예측치를 가중된 다수결 방식으로 조합한다. 이에 대해 그림 14.1에 그려져 있다. <br>\n",
    "Once all the classifiers have been trained, their predictions are then combined through a weighted majority voting scheme, as illustrated schematically in Figure 14.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터가 입력 벡터 x1, ..., xN과 해당 바이너리 대상 변수 t1, ..., tN (여기서 tn ∈ {−1, 1})을 포함하는 2- 클래스 분류 문제를 생각해보십시오. <br>\n",
    "2클래스 분류 문제를 고려해 보자. 훈련 데이터는 입력 벡터 x1, ..., xN과 해당 이진 타깃 변수 t1, ..., tN으로 구성되어 있으며, 이 때 $t_n \\in \\{-1, 1\\}$이다. <br>\n",
    "Consider a two-class classification problem, in which the training data comprises input vectors x1,...,xN along with corresponding binary target variables t1,...,tN where $t_N \\in \\{-1, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 데이터 포인트에는 연관된 가중치 매개 변수 wn이 주어지며, 이는 처음에 모든 데이터 포인트에 대해 1 / N으로 설정됩니다. <br>\n",
    "각 데이터 포인트들은 가중 매개변수 wn을 부여받으며, 매개변수들은 1/N으로 초기화된다. <br>\n",
    "Each data point is given an associated weighting parameter wn, which is initially set 1/N for all data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중 데이터를 사용하여 함수 $y(x) \\in \\{-1, 1\\}$을 제공하는 기본 분류기를 훈련 할 수있는 절차가 있다고 가정합니다. <br>\n",
    "가중된 데이터를 바탕으로 기저 분류기를 훈련시켜서 $y(x) \\in \\{-1, 1\\}$을 내놓을 수 있는 방법은 이미 존재한다고 가정할 것이다. <br>\n",
    "We shall suppose that we have a procedure available for training a base classifier using weighted data to give a function $y(x) \\in \\{-1, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알고리즘의 각 단계에서 AdaBoost는 잘못 분류 된 데이터 포인트에 더 큰 가중치를 부여하기 위해 이전에 훈련 된 분류기의 성능에 따라 가중치 계수가 조정되는 데이터 세트를 사용하여 새로운 분류기를 훈련합니다. <br>\n",
    "에이다부스트의 각 단계에서는 수정된 가중치를 바탕으로 분류기를 훈련시킨다. 이 때 이전에 훈련된 분류기의 결과에서 오분류된 데이터 포인트들에 더 높은 가중치를 부여하는 방식으로 가중치를 수정하게 된다. <br>\n",
    "At each stage of the algorithm, AdaBoost trains a new classifier using a data set in which the weighting coefficiencs are adjusted according to the performance of the previously trained classifers so as to give greater weight to the misclassified data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 원하는 수의 기본 분류 기가 훈련되면 결합되어 서로 다른 기본 분류기에 다른 가중치를 부여하는 계수를 사용하여위원회를 구성합니다. <br>\n",
    "원하는 수만큼의 기저 분류기를 훈련시키고 나면 이들을 조합해서 위원회를 구성한다. 위원회 구성 가정에서 각 기저 분류기들에 대해 서로 다른 가중치를 사용하게 된다. <br>\n",
    "Finally, when the desired number of base classifiers have been trained, they are combined to form a committee using coefficients that give different weight to different base classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost 알고리즘의 정확한 형식은 다음과 같습니다. <br>\n",
    "에이다부스트 알고리즘의 정확한 형태를 다음에 적어 두었다.  <br>\n",
    "The precise form of the AdaBoost alogrithm is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost\n",
    "1. 데이터 가중치 $\\{w_n\\}$을 $w_n^{(1)} = \\frac{1}{N}$으로 초기화한다. 이 때 n = 1,...,N이다. <br>\n",
    "2. m = 1,...,M에 대해 다음을 시행한다.  <br>\n",
    "    (a) 다음의 가중 오류 함수를 최소화하는 방식으로 분류기 $y_m(x)$를 훈련 데이터에 피팅한다. \n",
    "    $$J_m = \\sum_{n=1}^{N} w_n^{(m)}I(y_m(x_n) \\ne t_n)$$    \n",
    "      여기서 $I(y_m(x_n) \\ne t_n)$은 표시 함수로서 $y_m(x_n) \\ne t_n$이면 1, 아니면 0 값을 가진다. <br>\n",
    "    (b) 다음의 값을 계산한다.  <br>\n",
    "    $$\\epsilon_m = \\frac {\\sum_{n=1}^{N}w_n^{(m)}I(y_m(x_n) \\ne t_n)} {\\sum_{n=1}^{N}w_n^{(m)}} $$\n",
    "    그리고 이를 이용해서 다음을 계산한다.  <br>\n",
    "    $$\\alpha_m = \\ln \\big\\{\\frac{1 - \\epsilon_m} {\\epsilon_m} \\big\\} $$\n",
    "    (c) 데이터 가중 계수를 업데이트한다.  <br>\n",
    "    $$ w_n^{(m+1)} = w_n^{(m)} exp\\{\\alpha_m I(y_m(x_n) \\ne t_n) \\} $$\n",
    "3. 최종 모델을 이용해서 다음과 같이 예측을 시행한다.  <br>\n",
    "$$ Y_M(x) = sign \\big(\\sum_{m=1}^{M} \\alpha_my_m(x)\\big) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost <br>\n",
    "1. Initialize the data weighting coefficients $\\{w_n\\}$ by setting $w_n^{(1)} = \\frac{1}{N}$ for $n = 1,\\dots,N$. <br>\n",
    "2. For $m = 1,\\dots,M:$ <br>\n",
    "    (a) Fit a classifier $y_m(x)$ to the training data by minimizing the weighted error function <br>\n",
    "    $$J_m = \\sum_{n=1}^{N} w_n^{(m)}I(y_m(x_n) \\ne t_n)$$    \n",
    "    where $I(y_m(x_n) \\ne t_n)$ is the indicator function and equals 1 when $y_m(x_n) \\ne t_n$ and 0 otherwise. <br>\n",
    "    (b) Evaluate the quantities\n",
    "    $$\\epsilon_m = \\frac {\\sum_{n=1}^{N}w_n^{(m)}I(y_m(x_n) \\ne t_n)} {\\sum_{n=1}^{N}w_n^{(m)}} $$\n",
    "      and then use these to evaluate <br>\n",
    "    $$\\alpha_m = \\ln \\big\\{\\frac{1 - \\epsilon_m} {\\epsilon_m} \\big\\} $$\n",
    "    (c) Update the data weighting coefficients\n",
    "    $$ w_n^{(m+1)} = w_n^{(m)} exp\\{\\alpha_m I(y_m(x_n) \\ne t_n) \\} $$\n",
    "3. Make predictions using the final model, which is given by\n",
    "    $$ Y_M(x) = sign \\big(\\sum_{m=1}^{M} \\alpha_my_m(x)\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 기본 분류기 y1 (x)는 모두 동일한 가중치 계수 wn(1)를 사용하여 훈련되었으므로 단일 분류기를 훈련하기위한 일반적인 절차에 해당합니다. <br>\n",
    "첫 번째 기저 분류기 $y_1(x)$는 모든 가중치 $w_n(1)$ 값이 같은 상태에서 훈련된다. 따라서 단일 분류기를 훈련시키기 위한 보통의 과정을 그대로 따르게 된다. <br>\n",
    "We see that the first base classifier y1(x) is trained using weighting coefficients wn(1), that are all equal, which therefore corresponds to the usual procedure for training a single classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(14.18)에서 우리는 후속 반복에서 가중치 계수 wn(m)가 잘못 분류 된 데이터 포인트에 대해 증가하고 올바르게 분류된 데이터 포인트에 대해 감소한다는 것을 알 수 있습니다. <br>\n",
    "식 14.18을 보면 다음 반복에서부터는 가중 계수 $w_n^{(m)}$의 값이 오분류된 데이터 포인트들에 대해서는 증가하고, 올바르게 분류된 데이터 포인트들에 대해서는 변하지 않는 것을 볼 수 있다. <br>\n",
    "From (14.18), we see that in subsequent iterations the weighting coefficients wn(m) are increased for data points that are misclassified and unchanged for data points that are correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 연속적인 분류자는 이전 분류 자에 의해 잘못 분류 된 포인트를 더 강조해야하며, 연속적인 분류 자에 의해 계속 잘못 분류 된 데이터 포인트는 더 큰 가중치를받습니다. <br>\n",
    "따라서 다음에 연속되는 분류기들은 이전 분류기들에 의해 오분류된 데이터 포인트들을 더 강조해서 훈련을 시행하게 될 것이다. 그리고 계속해서 오분류되는 데이터 포인트들은 더 큰 가중치를 얻게 될 것이다. <br>\n",
    "Successive classifiers are therefore forced to place greater emphasis on points that have been misclassified by previous classifiers, and data points that continue to be misclassified by successive classifiers recieve ever greater weight.\n",
    "Successive classifiers are therefore forced to place greater emphasis on points that have been misclassified by previous classifiers, and data points that continue to be misclassified by successive classifiers receive ever greater weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "양 em은 데이터 세트에있는 각 기본 분류기의 오류율에 대한 가중 측정 값을 나타냅니다. <br>\n",
    "$\\epsilon_m$은 각 분류기의 데이터 집합에 대한 가중된 오류율을 나타낸다. <br>\n",
    "The quantities em represent weighted measures of the error rates of each of the base classifiers on the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 (14.17)에 의해 정의 된 가중치 계수 αm은 (14.19)에 의해 주어진 전체 출력을 계산할 때 더 정확한 분류기에 더 큰 가중치를 부여합니다. <br>\n",
    "식 14.17에 정의된 가중계수 $\\alpha_m$은 더 정확한 분류기에 대해 더 큰 값을 가지게 될 것이다. 이 값을 바탕으로 식 14.19에서 전체 출력값이 계산된다. <br>\n",
    "We therefore see that the weighting coefficients am defined by (14.17) give greater weight to the more accurate classifiers when computing the overall output given by (14.19).\n",
    "We therefore see that the weighting coefficients αm defined by (14.17) give greater weight to the more accurate classifiers when computing the overall output given by (14.19)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost 알고리즘은 그림 A.7에 표시된 장난감 분류 데이터 집합에서 가져온 30 개 데이터 포인트의 하위 집합을 사용하여 그림 14.2에 나와 있습니다.  <br>\n",
    "에이다부스트 알고리즘의 예시가 그림 14.2에 그려져 있다. 그림 A.7의 분류 데이터 집합에서 30개의 데이터 포인트를 표본 추출해서 사용하였다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 각 기본 학습자는 입력 변수 중 하나에 대한 임계 값으로 구성됩니다. <br>\n",
    "여기서 각각의 기저 학습기는 각각의 입력 변수에 대한 임계값으로 이루어져 있다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 간단한 분류기는 '결정 스텀프 (decision stumps)', 즉 단일 노드가있는 결정 트리로 알려진 결정 트리의 한 형태에 해당합니다. <br>\n",
    "이 단순한 분류기는 단일 노드를 가진 의사 결정 트리에 해당한다. 이를 의사 결정 그루터기라 하기도 한다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 각 기본 학습자는 입력 특성 중 하나가 임계 값을 초과하는지 여부에 따라 입력을 분류하므로 공간을 축 중 하나에 평행 한 선형 결정 표면으로 분리 된 두 영역으로 분할합니다. <br>\n",
    "각각의 기저 학습기는 입력 특징들 중 하나가 어떤 임계값을 넘었는지 아닌지를 바탕으로 입력값을 분류한다. 그 결과 ㅇ축들 중 하나에 직교하는 선형 결정 경계를 이용해서 공간을 단순하게 두 구역으로 나누게 된다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.1 지수 오류의 최소화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
